# Dataset
StoresReview <- read_excel("GRUPPO 3-4-5. Industry elettronica.xlsx")
# Aggiunta Primary key
StoresReview$ID <- seq(1:nrow(StoresReview))
# Dataset con sole recensioni in italiano.
Ita_StoresReview <- StoresReview[(StoresReview$lang_value == "it" |
is.na(StoresReview$lang_value) == TRUE) &
is.na(StoresReview$text) == FALSE,]
# Corpus con i testi NON vuoti
Corpus_Totale <- corpus(Ita_StoresReview)
attr(Corpus_Totale, "docvars")$ID
# NON PULISCE TUTTO. !!, emoji
Dfm_Totale <- dfm(tokens(Corpus_Totale,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(c(stopwords("italian"))) %>%
tokens_wordstem(language = "italian")) %>%
dfm_trim(min_termfreq = 10,
min_docfreq = 2)
# ANALISI ----
# Suddivisione dataset per social
Tweet_ita <- Ita_StoresReview[Ita_StoresReview$social == "twitter",]
Places_ita <- Ita_StoresReview[Ita_StoresReview$social == "places",]
# Corpus per i Tweet
Tweet_Corpus <- corpus(Tweet_ita)
# Foreign key impostate
attr(Tweet_Corpus, "docvars")$ID
# Corpus per i Places
Places_Corpus <- corpus(Places_ita)
docnames(Training_tweet) <- paste0("new_", docnames(Training_tweet))
Training_data <- c(Training_tweet, Training_places)
# Campionamento con numerosità 200
set.seed(001)
Training_places <- sample(Places_Corpus, size = 160, replace = FALSE)
set.seed(002)
Training_tweet <- sample(Tweet_Corpus, size = 40, replace = FALSE)
docnames(Training_tweet) <- paste0("new_", docnames(Training_tweet))
Training_data <- c(Training_tweet, Training_places)
# Corpus per il TEST SET
Test_data <- Corpus_Totale[!(Corpus_Totale %in% Training_data)]
Campione <- data.frame(
attr(Training_data, "docvars")$ID,
Persona <- rep(c("William","Davide","Maddalena","Giacomo"),each = 50),
Training_data,
Sentiment <- NA)
names(Campione) <- c("ID","Persona","text","sentiment")
Test_data <- data.frame(
attr(Test_data,"docvars")$ID,
Test_data
)
names(Test_data) <- c("ID","text")
Campione <- read_excel("Training Data Grezzo.xlsx")
Campione$sentiment <- ifelse(Campione$sentiment == -1, "Negativo",
ifelse(Campione$sentiment == 0, "Neutro",
"Positivo"))
Training_data <- corpus(Campione)
Test_Corpus <- corpus(Test_data)
Dfm_Training <- dfm(tokens(Training_data,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(c(stopwords("italian"))) %>%
tokens_wordstem(language = "italian")) %>%
dfm_trim(min_termfreq = 10,
min_docfreq = 2)
Dfm_Test <- dfm(tokens(Test_Corpus,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(c(stopwords("italian"))) %>%
tokens_wordstem(language = "italian")) %>%
dfm_trim(min_termfreq = 10,
min_docfreq = 2)
Dfm_Test <- dfm_match(Dfm_Test,
features = featnames(Dfm_Training))
Matrice_Training <- as.matrix(Dfm_Training)
Matrice_Test <- as.matrix(Dfm_Test)
Dfm_Training@docvars$sentiment <- as.factor(Dfm_Training@docvars$sentiment)
set.seed(123)
system.time(NaiveBayesModel <- multinomial_naive_bayes
(x=Matrice_Training,
y=Dfm_Training@docvars$sentiment,
laplace = 1))
summary(NaiveBayesModel)
Test_predictedNB <- predict(NaiveBayesModel,
Matrice_Test)
View(Test_data)
# Check list ----
str(Test_predictedNB)
str(Matrice_Test)
view(Matrice_Test)
View(Matrice_Test)
View(Campione)
View(Matrice_Training)
View(Test_data)
View(Campione)
Test_predictedNB <- predict(NaiveBayesModel,
Matrice_Test)
library(readxl)
library(writexl)
library(rstudioapi)
library(quanteda)
library(quanteda.textstats)
library(naivebayes)
library(ggplot2)
# Directory della cartella condivisa
setwd(dirname(getActiveDocumentContext()$path))
# Dataset
StoresReview <- read_excel("GRUPPO 3-4-5. Industry elettronica.xlsx")
# Aggiunta Primary key
StoresReview$ID <- seq(1:nrow(StoresReview))
# Dataset con sole recensioni in italiano.
Ita_StoresReview <- StoresReview[(StoresReview$lang_value == "it" |
is.na(StoresReview$lang_value) == TRUE) &
is.na(StoresReview$text) == FALSE,]
# Corpus con i testi NON vuoti
Corpus_Totale <- corpus(Ita_StoresReview)
# NON PULISCE TUTTO. !!, emoji
Dfm_Totale <- dfm(tokens(Corpus_Totale,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(c(stopwords("italian"))) %>%
tokens_wordstem(language = "italian")) %>%
dfm_trim(min_termfreq = 10,
min_docfreq = 2)
# ANALISI ----
# Suddivisione dataset per social
Tweet_ita <- Ita_StoresReview[Ita_StoresReview$social == "twitter",]
Places_ita <- Ita_StoresReview[Ita_StoresReview$social == "places",]
# Corpus per i Tweet
Tweet_Corpus <- corpus(Tweet_ita)
# Corpus per i Places
Places_Corpus <- corpus(Places_ita)
# Campionamento con numerosità 200
set.seed(001)
Training_places <- sample(Places_Corpus, size = 160, replace = FALSE)
set.seed(002)
Training_tweet <- sample(Tweet_Corpus, size = 40, replace = FALSE)
docnames(Training_tweet) <- paste0("new_", docnames(Training_tweet))
Training_data <- c(Training_tweet, Training_places)
# Corpus per il TEST SET
Test_data <- Corpus_Totale[!(Corpus_Totale %in% Training_data)]
Campione <- data.frame(
attr(Training_data, "docvars")$ID,
Persona <- rep(c("William","Davide","Maddalena","Giacomo"),each = 50),
Training_data,
Sentiment <- NA)
names(Campione) <- c("ID","Persona","text","sentiment")
Test_data <- data.frame(
attr(Test_data,"docvars")$ID,
Test_data
)
names(Test_data) <- c("ID","text")
#Esportare il Campione
#write_xlsx(Campione, "Training Data Grezzo.xlsx") # NON RUNNARE !!!!!!!!!!!!!!!!!!!!!!!
Campione <- read_excel("Training Data Grezzo.xlsx")
Campione$sentiment <- ifelse(Campione$sentiment == -1, "Negativo",
ifelse(Campione$sentiment == 0, "Neutro",
"Positivo"))
Training_data <- corpus(Campione)
Test_Corpus <- corpus(Test_data)
Dfm_Training <- dfm(tokens(Training_data,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(c(stopwords("italian"))) %>%
tokens_wordstem(language = "italian")) %>%
dfm_trim(min_termfreq = 10,
min_docfreq = 2)
Dfm_Test <- dfm(tokens(Test_Corpus,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(c(stopwords("italian"))) %>%
tokens_wordstem(language = "italian")) %>%
dfm_trim(min_termfreq = 10,
min_docfreq = 2)
Dfm_Test <- dfm_match(Dfm_Test,
features = featnames(Dfm_Training))
Matrice_Training <- as.matrix(Dfm_Training)
Matrice_Test <- as.matrix(Dfm_Test)
Dfm_Training@docvars$sentiment <- as.factor(Dfm_Training@docvars$sentiment)
set.seed(123)
system.time(NaiveBayesModel <- multinomial_naive_bayes
(x=Matrice_Training,
y=Dfm_Training@docvars$sentiment,
laplace = 1))
summary(NaiveBayesModel)
Test_predictedNB <- predict(NaiveBayesModel,
Matrice_Test)
View(NaiveBayesModel)
# 1: Pulizia e Preparazione dei dati ----
install.packages("randomForest")
library(randomForest)
set.seed(150)
system.time(RF <- randomForest(y= Dfm_Training@docvars$sentiment,
x= Matrice_Training,
importance=TRUE,
do.trace=FALSE,
ntree=500))
RF
mode(RF)
str(RF)
str(RF$predicted)
RF$predicted
str(RF)
install.packages("iml")
install.packages("future")
install.packages("future.callr")
install.packages("e1071")
library(iml)
library(future)
library(future.callr)
library(e1071)
set.seed(175)
system.time(SupportVectorMachine <- svm(
y= Dfm_Training@docvars$sentiment,
x=Matrice_Training, kernel='linear', cost = 1))
length(SupportVectorMachine$index)
system.time(Test_predictedSV <- predict(SupportVectorMachine,
Matrice_Test))
Test_predictedSV
View(Test_data)
Test_data$Bayes <- Test_predictedNB
View(Test_data)
Test_data$Forest <- RF$predicted
Test_data$Support <- Test_predictedSV
View(Test_data)
View(Campione)
results <- as.data.frame(rbind(prop.table(table(Test_predictedNB)),
prop.table(table(Test_predictedRF)),
prop.table(table(Test_predictedSV))))
RF
RF
table(Campione$sentiment)
(6+17+10+17+4+2)/200
plot(RF, type = "l", col = c("black", "steelblue4","violetred4", "springgreen4"),
main = "Random Forest Model Errors: sentiment variable")
plot(RF, type = "l", col = c("black", "steelblue4","violetred4", "springgreen4"),
main = "Random Forest Model Errors: sentiment variable")
legend("topright", horiz = F, cex = 0.7,
fill = c("springgreen4", "black", "steelblue4", "violetred4"),
c("Positive error", "Average error", "Negative error", "Neutral error"))
Errori <- as.data.frame(RF$err.rate)
which.min(Errori$OOB) # 96
set.seed(150)
system.time(RF <- randomForest(y= Dfm_Training@docvars$sentiment,
x= Matrice_Training,
importance=TRUE,
do.trace=FALSE,
ntree=53))
RF
plot(RF, type = "l", col = c("black", "steelblue4","violetred4", "springgreen4"),
main = "Random Forest Model Errors: sentiment variable")
legend("topright", horiz = F, cex = 0.7,
fill = c("springgreen4", "black", "steelblue4", "violetred4"),
c("Positive error", "Average error", "Negative error", "Neutral error"))
system.time(Test_predictedRF <- predict(RF,
Matrice_Test ,type="class"))
Test_data$Forest <- Test_predictedRF
View(Test_data)
results <- as.data.frame(rbind(prop.table(table(Test_predictedNB)),
prop.table(table(Test_predictedRF)),
prop.table(table(Test_predictedSV))))
View(results)
library(reshape2)
# 1: Pulizia e Preparazione dei dati ----
install.packages("reshape2")
library(reshape2)
results$algorithm <- c("Naive Bayes", "Random Forest", "Support Vector Machine")
df.long<-melt(results,id.vars=c("algorithm"))
View(df.long)
ggplot(df.long,aes(algorithm,value,fill=variable))+
geom_bar(position="dodge",stat="identity") + scale_fill_manual(values = c("violetred3", "yellow3", "orange2")) +
labs(title = "Comparazione delle predizioni") +
theme(axis.text.x = element_text(color="#993333", angle=90)) + coord_flip() +
ylab(label="Proporzione delle categorie nel test set") + xlab("Algoritmi") +
guides(fill=guide_legend(title="Categorie di \nsentiment")) +
theme(plot.title = element_text(color = "black", size = 12, face = "plain"),
axis.title=element_text(size=11,face="plain"),
axis.text= element_text(size =10, face = "italic")
)
ggplot(df.long,aes(algorithm,value,fill=variable))+
geom_bar(position="dodge",stat="identity") + scale_fill_manual(values = c("violetred3", "yellow3", "orange2")) +
labs(title = "Comparazione delle predizioni") +
theme(axis.text.x = element_text(color="#993333", angle=90)) + coord_flip() +
ylab(label="Proporzione delle categorie nel test set") + xlab("Algoritmi") +
guides(fill=guide_legend(title="Categorie di \nsentiment")) +
theme(plot.title = element_text(color = "black", size = 12, face = "plain"),
axis.title=element_text(size=11,face="plain"),
axis.text= element_text(size =10, face = "italic")
)
summary(NaiveBayesModel)
# 1: Pulizia e Preparazione dei dati ----
install.packages(cvTools)
# 1: Pulizia e Preparazione dei dati ----
install.packages("cvTools")
install.packages("caret")
library(caret)
library(cvTools)
install.packages("robustbase")
install.packages("robustbase")
Matrice_Training2 <- Matrice_Training
set.seed(200)
#Definiamo un oggetto k che indichi il numero di folders
k <- 5
#Dividiamo la matrice in k folders
folds <- cvFolds(NROW(Matrice_Training2), K = k)
system.time(for(i in 1:k){
Matrice_Training <-
Matrice_Training2 [folds$subsets[folds$which != i], ]
ValidationSet <-
Matrice_Training2 [folds$subsets[folds$which == i], ]
set.seed(200)
NaiveBayesModel <- multinomial_naive_bayes(
y= Dfm_Training[folds$subsets[folds$which != i], ]
@docvars$sentiment ,
x=Matrice_Training, laplace = 1)
Predictions_NB <- predict(NaiveBayesModel,
newdata = ValidationSet,
type = "class")
class_table <- table("Predictions"= Predictions_NB,
"Actual"=Dfm_Training[folds$subsets[folds$which == i], ]@docvars$sentiment)
print(class_table)
df<-confusionMatrix( class_table, mode = "everything")
df_measures_NB<-paste0("conf.mat.nb",i)
assign(df_measures_NB,df)
})
NB_Prediction <- data.frame(col1=vector(), col2=vector(), col3=vector(), col4=vector())
for(i in mget(ls(pattern = "conf.mat.nb")) ) {
Accuracy <-(i)$overall[1]
p <- as.data.frame((i)$byClass)
F1_negative <- p$F1[1]
F1_neutral <- p$F1[2]
F1_positive <- p$F1[3]
NB_Prediction <- rbind(NB_Prediction , cbind(Accuracy , F1_negative ,
F1_neutral, F1_positive ))
}
#guardiamo la struttura
str(NB_Prediction)
NB_Prediction [is.na(NB_Prediction )] <- 0
#guardiamo la struttura
str(NB_Prediction)
AverageAccuracy_NB <- mean(NB_Prediction[, 1] )
AverageF1_NB<- mean(colMeans(NB_Prediction[-1] ))
AverageAccuracy_NB
AverageF1_NB
system.time(for(i in 1:k){
Matrice_Training <-
Matrice_Training2 [folds$subsets[folds$which != i], ]
ValidationSet <-
Matrice_Training2 [folds$subsets[folds$which == i], ]
set.seed(250)
RandomForest <- randomForest(
y= Dfm_Training[folds$subsets[folds$which != i], ]
@docvars$sentiment ,
x=Matrice_Training, do.trace=FALSE, ntree=53)
Predictions_RF <- predict(RandomForest,
newdata= ValidationSet,
type="class")
class_table <- table("Predictions"= Predictions_RF,
"Actual"=Dfm_Training[folds$subsets[folds$which == i], ]@docvars$sentiment)
print(class_table)
df<-confusionMatrix( class_table, mode = "everything")
df_measures_RF<-paste0("conf.mat.rf",i)
assign(df_measures_RF,df)
})
RF_Predictions <- data.frame(col1=vector(), col2=vector(), col3=vector(), col4 = vector())
for(i in mget(ls(pattern = "conf.mat.rf")) ) {
Accuracy <-(i)$overall[1]
p <- as.data.frame((i)$byClass)
F1_negative <- p$F1[1]
F1_neutral <- p$F1[2]
F1_positive <- p$F1[3]
RF_Predictions <- rbind(RF_Predictions , cbind(Accuracy , F1_negative ,
F1_neutral, F1_positive ))
}
str(RF_Predictions)
RF_Predictions [is.na(RF_Predictions )] <- 0
#Calcoliamo i valori medi
AverageAccuracy_RF <- mean(RF_Predictions[, 1] )
AverageF1_RF<- mean(colMeans(RF_Predictions[-1] ))
AverageAccuracy_RF
AverageF1_RF
system.time(for(i in 1:k){
Matrice_Training <-
Matrice_Training2 [folds$subsets[folds$which != i], ]
ValidationSet <-
Matrice_Training2 [folds$subsets[folds$which == i], ]
set.seed(300)
SupportVectorMachine<- svm(
y= Dfm_Training[folds$subsets[folds$which != i], ]
@docvars$sentiment,
x=Matrice_Training, kernel='linear', cost = 1)
Prediction_SVM <- predict(SupportVectorMachine,
newdata=ValidationSet)
class_table <- table("Predictions"= Prediction_SVM,
"Actual"=Dfm_Training[folds$subsets[folds$which == i], ]@docvars$sentiment)
print(class_table)
df<-confusionMatrix( class_table, mode = "everything")
df_measures_SVM<-paste0("conf.mat.sv",i)
assign(df_measures_SVM,df)
})
#Riempiamo il dataframe
for(i in mget(ls(pattern = "conf.mat.sv")) ) {
Accuracy <-(i)$overall[1]
p <- as.data.frame((i)$byClass)
F1_negative <- p$F1[1]
F1_neutral <- p$F1[2]
F1_positive <- p$F1[3]
SVM_Prediction <- rbind(SVM_Prediction , cbind(Accuracy , F1_negative ,
F1_neutral, F1_positive ))
}
SVM_Prediction <- data.frame(col1=vector(), col2=vector(), col3=vector(), col4=vector())
#Riempiamo il dataframe
for(i in mget(ls(pattern = "conf.mat.sv")) ) {
Accuracy <-(i)$overall[1]
p <- as.data.frame((i)$byClass)
F1_negative <- p$F1[1]
F1_neutral <- p$F1[2]
F1_positive <- p$F1[3]
SVM_Prediction <- rbind(SVM_Prediction , cbind(Accuracy , F1_negative ,
F1_neutral, F1_positive ))
}
str(SVM_Prediction)
SVM_Prediction [is.na(SVM_Prediction)] <- 0
#Calcoliamo i valori medi
AverageAccuracy_SVM <- mean(SVM_Prediction[, 1] )
AverageF1_SVM<- mean(colMeans(SVM_Prediction[-1] ))
AverageAccuracy_SVM
AverageF1_SVM
AccNB <- as.data.frame(AverageAccuracy_NB )
colnames(AccNB)[1] <- "NB"
#Creo un dataframe per RF
AccRF <- as.data.frame(AverageAccuracy_RF )
#Rinomino la colonna
colnames(AccRF)[1] <- "RF"
#Creo un dataframe per SVM
AccSVM<- as.data.frame(AverageAccuracy_SVM )
#Rinomino la colonna
colnames(AccSVM)[1] <- "SVM"
#Unisco in un unico dataframe i valori di accuracy dei tre modelli
Accuracy_models <- cbind(AccNB, AccRF, AccSVM)
Accuracy_models
Accuracy_models_Melt <-melt(Accuracy_models)
str(Accuracy_models_Melt)
Accuracy_models_Melt <-melt(Accuracy_models)
View(Accuracy_models_Melt)
View(df.long)
Accuracy_models_Melt <-melt(Accuracy_models, id.vars = c("variable"))
Accuracy_models_Melt <-melt(Accuracy_models, id.vars = "variable")
Accuracy_models_Melt <-melt(Accuracy_models)
str(Accuracy_models_Melt)
plot_accuracy <- ggplot(Accuracy_models_Melt, aes(x=variable, y=value, color = variable)) +
geom_boxplot() + xlab("Algorithm") + ylab(label="Values of accuracy") +
labs(title = "Cross-validation with k =5: values of accuracy") + coord_flip() +
theme_bw() +
guides(color=guide_legend(title="Algorithms")) +
theme(plot.title = element_text(color = "black", size = 12, face = "italic"),
axis.title.x =element_text(size=12,face="bold"),
axis.title.y =element_text(size=12, face = "plain"),
axis.text= element_text(size =10, face = "italic")
)
plot_accuracy
F1NB <- as.data.frame(AverageF1_NB)
colnames(F1NB)[1] <- "NB"
#RF
F1RF<- as.data.frame(AverageF1_RF )
colnames(F1RF)[1] <- "RF"
#SVM
F1SVM <- as.data.frame(AverageF1_SVM)
colnames(F1SVM)[1] <- "SVM"
#DATAFRAME
f1_models <- cbind(F1NB, F1RF, F1SVM)
f1_models
f1_models_melt <-melt(f1_models)
str(f1_models_melt)
plot_f1 <- ggplot(f1_models_melt, aes(x=variable, y=value, color = variable)) +
geom_boxplot() + xlab("Algorithm") + ylab(label="Values of f1") +
labs(title = "Cross-validation with k =5: values of f1") + coord_flip() +
theme_bw() +
guides(color=guide_legend(title="Algorithms")) +
theme(plot.title = element_text(color = "black", size = 12, face = "italic"),
axis.title.x =element_text(size=12,face="bold"),
axis.title.y =element_text(size=12, face = "plain"),
axis.text= element_text(size =10, face = "italic")
)
# 1: Pulizia e Preparazione dei dati ----
install.packages("gridExtra")
library(gridExtra)
grid.arrange(plot_accuracy, plot_f1, nrow=2)
View(Test_data)
View(Ita_StoresReview)
# Top parole del DFM
topfeatures(Dfm_Totale,100)
# Top parole del DFM
topfeatures(Dfm_Totale,300)
View(StoresReview)
